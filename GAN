import numpy as np
import math as m
from chainer import functions as F
from chainer import links as L
from chainer import Chain, Variable, optimizers, iterators, config
import matplotlib.pyplot as plt
from mnist import MNIST


class GeneratorDistribution(object):

    def __init__(self, mean=0, sigma=0.5):
        self.mean = mean
        self.sigma = sigma

    def sample(self,N=1):
        data = np.random.randn(N)*self.sigma + self.mean
        return np.array(data,dtype='float32')


class Generator(Chain):

    def __init__(self, input_dims=100, feature_dims=100, conv_hidden1=28, conv_hidden2=28, hidden1=100, hidden2=49, batch_size=[1]):

        # inherit from Chain
        super(Generator, self).__init__()

        # define layers
        with self.init_scope():
            self.l1 = L.Linear(None, hidden1)
            self.l2 = L.Linear(None, hidden2)
            self.l3 = L.Convolution2D(None, 4,1)
            self.l4 = L.Convolution2D(None, 8,1)
            self.l5 = L.Convolution2D(None, 16,1)

        # add variables to class
        self.feature_dims = feature_dims
        self.hidden_size = [hidden1, hidden2, conv_hidden1, conv_hidden2]
        self.output_size = feature_dims
        self.batch_size = batch_size

        self.reset()

    def __call__(self, x):

        # process data
        try:
            x[0][0]
        except:
            x = np.expand_dims(x, 0)  # add a fake dimension on x

        # propagate input through layers
        h0 = F.leaky_relu(self.l1(x))
        h1 = F.leaky_relu(self.l2(h0))
        # data processing for convolution
        h1.data = np.reshape(h1.data,[self.batch_size[0],7, 7])
        h1.data = np.expand_dims(h1.data,axis=1).astype('float32')
        h2 = F.leaky_relu(self.l3(h1))
        h3 = F.leaky_relu(self.l4(h2))
        h4 = F.leaky_relu(self.l5(h3))

        return h4

    def reset(self):
        pass


class Discriminator(Chain):

    def __init__(self, feature_dims, conv_hidden1 = 28, conv_hidden2 = 50, conv_hidden3 = 100, hidden1=100, hidden2=50, hidden3=10, output=1, batch_size=1):

        # inherit from Chain
        super(Discriminator, self).__init__()

        # define layers
        with self.init_scope():
            self.l1 = L.Convolution2D(None, conv_hidden1, 1)
            self.l2 = L.Convolution2D(None, conv_hidden2, 1)
            self.l3 = L.Convolution2D(None, conv_hidden3, 1)
            self.l4 = L.Linear(None, hidden1)
            self.l5 = L.Linear(None, hidden2)
            self.l6 = L.Linear(None, 10)
            self.l7 = L.Linear(None, output)

        # add variables to class
        self.feature_dims = feature_dims
        self.hidden = [conv_hidden1, conv_hidden2, conv_hidden3, hidden1, hidden2, hidden3]
        self.output = output
        self.batch_size = batch_size

        self.reset()

    def __call__(self,x):

        # change x type to Chainer variable
        x = Variable(x)

        # process data
        try:
            x[0][0]
        except:
            x = np.expand_dims(x, 0)  # add a fake dimension on x

        # define number of channels for convolution
        x.data = np.reshape(x.data, [self.batch_size, 28, 28])
        x.data = np.expand_dims(x.data, axis=1).astype('float32')

        # propagate input through layers
        h0 = F.leaky_relu(self.l1(x))
        h1 = F.leaky_relu(self.l2(h0))
        h2 = F.leaky_relu(self.l3(h1))
        h3 = F.leaky_relu(self.l4(h2))
        h4 = F.leaky_relu(self.l5(h3))
        h5 = F.tanh(self.l6(h4))
        h6 = F.sigmoid(self.l7(h5))

        return h6

    def reset(self):
        pass


def GAN(mndata=None, Generator=Generator, Discriminator=Discriminator, epochs=20, batch_size=200, feature_dims=1):

    # # init target distribution
    # distribution = GeneratorDistribution(mean=-3)

    if mndata:
        feature_dims = [batch_size,28,28]
    else:
        distribution = GeneratorDistribution(mean=-3)

    # calculate half batch
    assert not(batch_size%2)
    half_batch = feature_dims
    half_batch[0] //= 2

    # init data iterators
    train_iter = iterators.SerialIterator(mndata.train_images, half_batch[0])
    test_iter = iterators.SerialIterator(mndata.test_images, half_batch[0],
                                         repeat=False, shuffle=True)
    # init networks
    generator = Generator(feature_dims=feature_dims, input_dims=feature_dims, batch_size=half_batch)
    discriminator = Discriminator(feature_dims=feature_dims, output=1, batch_size=batch_size)

    # init optimizers
    gen_optimizer = optimizers.adam.Adam()
    disc_optimizer = optimizers.adam.Adam()
    gen_optimizer.setup(generator)
    disc_optimizer.setup(discriminator)

    labels = np.zeros(batch_size)

    iter = 0
    # training loop
    while train_iter.epoch < epochs:

        iter += 1
        ## prepare data for discriminator


        # get generator data
        data1 = np.ndarray(half_batch)

        gen_data = generator(np.random.random(size=feature_dims).astype('float32'))
        data1 = gen_data.data
        data1 = np.reshape(data1,(feature_dims))

        if not(iter%6):
            plt.imshow(data1[0],cmap='gray_r')

        # if not i%100:
        #     # visualize generator distribution
        #     plt.clf()
        #     plt.hist(data1, bins='auto', orientation='vertical')
        #     plt.draw()

        # label generator data as fake
        label1 = np.zeros(half_batch[0])

        # get mnist data
        data2 = train_iter.next()
        data2 = np.array(data2)
        data2 = np.reshape(data2,feature_dims)

        # label mnist data as real
        label2 = np.ones(half_batch[0])

        # concatenate real and fake data into a single batch
        data = np.concatenate((data1, data2), axis=0)
        data = np.array(data).astype('float32')
        labels = np.concatenate((label1, label2),axis=0).astype('int32')
        labels = np.expand_dims(labels, axis=1)

        ### train discriminator

        discriminator.cleargrads()
        y = discriminator(data)  # discriminator response
        d_loss = F.sigmoid_cross_entropy(y, labels)  # calculate loss

        # Calculate the gradients in the network

        d_loss.backward()

        # update
        disc_optimizer.update()

        # prepare for new data
        data1 = np.ndarray(half_batch)
        data2 = np.ndarray(half_batch)

        # get fresh data from generator
        gen_data1 = generator(np.random.random(size=feature_dims).astype('float32'))
        gen_data2 = generator(np.random.random(size=feature_dims).astype('float32'))
        data1 = gen_data1.data
        data2 = gen_data2.data
        data1 = np.reshape(data1, (feature_dims))
        data2 = np.reshape(data2, (feature_dims))

        # label both data batches as fake
        label1 = np.zeros(half_batch[0])
        label2 = np.zeros(half_batch[0])

        # concatenate data into a single batch
        data = np.concatenate((data1, data2), axis=0)
        data = np.array(data).astype('float32')
        labels = np.concatenate((label1, label2),axis=0).astype('int32') # complete batch is fake!
        labels = np.expand_dims(labels, axis=1)

        ### train generator

        generator.cleargrads()
        y = discriminator(data)  # discriminator response after learning from batch
        g_loss = F.sigmoid_cross_entropy(y, labels)
        g_loss.backward()
        gen_optimizer.update()

        # # if final loop
        # if train_iter.is_new_epoch:
        #     print('epoch: ',train_iter.epoch, ',generator loss: ',g_loss.data, 'discriminator loss: ', d_loss.data)
        print('epoch: ', train_iter.epoch, ',generator loss: ', g_loss.data, 'discriminator loss: ', d_loss.data)
    return generator, discriminator

def visualize_generator_1D(generator_responses):
    pass

# data_range = GeneratorDistribution(mean=-3)
# samples = data_range.sample(10000)
# mygen=Generator(feature_dims=100,hidden_size=10,output=100)
# mydisc=Discriminator(feature_dims=100,hidden0=10,hidden1=10,hidden2=10,output=1)

# set interactive mode on
plt.ion()

# get Mnist data
mndata = MNIST('/home/mknull/python-mnist/data')
mndata.load_training()
mndata.load_testing()

batchsize=2000

config.debug = True
trained_generator, trained_discriminator = GAN(mndata=mndata, Generator=Generator, Discriminator=Discriminator, epochs=20, batch_size=batchsize)
#plt.imshow(np.reshape(training_data[1],[28,28]),cmap='gray_r')
